from utils.torch_playground import *

### ğŸŸ¢ ä»»åŠ¡ 1ï¼šåˆ›å»ºå¼ é‡ä¸ç±»å‹æ„ŸçŸ¥
print('----1.å¼ é‡åˆ›å»ºä¸Dtype----')

# åˆ›å»ºä¸åŒdtypeçš„å¼ é‡
# åˆ›å»ºé»˜è®¤å¼ é‡
with time_block("Create Default Tensor"):
    x_default = torch.randn(3, 3)
    inspect(x_default, name = "x_default")

# åˆ›å»ºæ•´æ•°å¼ é‡ å¿…é¡»æ˜¾å¼æŒ‡å®šdtypeä¸ºtorch.long
x_int = torch.randint(0, 10, (3, 3), dtype=torch.long)
inspect(x_int, name = "x_int") 

#è¿™é‡Œæœ‰å‘ å¦‚æœæŒ‡å®š torchä¼šè‡ªåŠ¨æ¨æ–­ä¸ºint
x_list = torch.tensor([1, 2, 3, 4])
inspect(x_list, name = "x_list")


# **ğŸ§  çŸ¥è¯†ç‚¹ï¼š**

# * **Float32**: ç¥ç»ç½‘ç»œçš„æ ‡å‡†è¡€æ¶²ã€‚
# * **Long (Int64)**: ç¥ç»ç½‘ç»œçš„æŒ‡è·¯ç‰Œï¼ˆç¬¬å‡ ç±»ã€ç¬¬å‡ ä¸ªè¯ï¼‰ã€‚
# **åˆ‡è®°ï¼šäº¤å‰ç†µæŸå¤±ï¼ˆCrossEntropyLossï¼‰çš„æ ‡ç­¾å¿…é¡»æ˜¯ Longï¼**
# * **Int32/Int16/Int8**: èŠ‚çœå†…å­˜çš„åˆ©å™¨ï¼Œä½†è¦å°å¿ƒæº¢å‡ºã€‚


## ğŸŸ¢ ä»»åŠ¡ 2ï¼šå¼ é‡è®¾å¤‡ç®¡ç†(cpu vs gpu)
# Tensor æ˜¯æœ‰â€œæˆ·å£â€çš„ã€‚åœ¨ CPU ä¸Šçš„ Tensor æ— æ³•å’Œ GPU ä¸Šçš„ Tensor ç›´æ¥è¿ç®—ï¼Œ
# ä¼šæŠ¥é”™ `Expected all tensors to be on the same device`ã€‚


print("\n--- 2. è®¾å¤‡æ¬è¿ (.to) ---")

# 1. åˆ›å»ºåœ¨CPUä¸Š
x_cpu = torch.zeros((2, 2))
inspect(x_cpu, name="x_cpu")

# 2. æ¬è¿åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
# è¿™é‡Œçš„DEVICEæ˜¯æˆ‘ä»¬ä»utils/torch_playground.pyè‡ªåŠ¨é€‰æ‹©çš„è®¾å¤‡
x_gpu = x_cpu.to(DEVICE)
inspect(x_gpu, name="x_gpu")

# 3. éªŒè¯ï¼šè®¾å¤‡ä¸åŒä¸èƒ½è®¡ç®—
try:
    _ = x_cpu + x_gpu
except RuntimeError as e:
    print(f"âœ… æ­£ç¡®æ•è·è®¾å¤‡ä¸åŒ¹é…é”™è¯¯: {e}")
    print("âš ï¸ æç¤ºï¼šç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨åŒä¸€è®¾å¤‡ä¸Šå†è¿›è¡Œè¿ç®—ï¼ï¼ˆéƒ½.to(DEVICE)ï¼‰")

# **ğŸ§  çŸ¥è¯†ç‚¹ï¼š**
# * `.to(device)` æ˜¯ä¸€ä¸ª**æ‹·è´**æ“ä½œï¼ˆå¦‚æœè®¾å¤‡ä¸åŒï¼‰ã€‚
# * æ•°æ®åŠ è½½é€šå¸¸å‘ç”Ÿåœ¨ CPUï¼ˆç¡¬ç›˜ -> å†…å­˜ï¼‰ï¼Œè®­ç»ƒå‘ç”Ÿåœ¨ GPUã€‚
# æ‰€ä»¥è®­ç»ƒå¾ªç¯é‡Œæ€»æœ‰ä¸€å¥ `inputs = inputs.to(device)`ã€‚


### ğŸŸ¢ ä»»åŠ¡ 3ï¼šå°å®éªŒ â€”â€” é€Ÿåº¦å¯¹æ¯” (CPU vs CUDA)
print("\n--- 3. æ€§èƒ½å®éªŒ: CPU vs GPU ---")
N = 10000
a_cpu = torch.randn((N, N))
b_cpu = torch.randn((N, N))

a_gpu = a_cpu.to(DEVICE)
b_gpu = b_cpu.to(DEVICE)

print(f"å¼€å§‹ CPU è®¡ç®— ({N}x{N})...")
with time_block("CPU Matrix Multiplication"):
    c_cpu = a_cpu @ b_cpu
    
print(f"å¼€å§‹ GPU è®¡ç®— ({N}x{N})...")
with time_block("GPU Matrix Multiplication"):
    c_gpu = a_gpu @ b_gpu
     # âš ï¸ å…³é”®ï¼šGPU æ˜¯å¼‚æ­¥æ‰§è¡Œçš„ï¼Œå¿…é¡»åŒæ­¥ç­‰å¾…æ‰€æœ‰å‘½ä»¤è·‘å®Œæ‰èƒ½æµ‹å‡ºçœŸå®æ—¶é—´
    torch.cuda.synchronize() 


