import torch

def print_grad_info(tensor, name):
    """è¾…åŠ©å‡½æ•°ï¼šæ‰“å°å¼ é‡çš„æ¢¯åº¦ä¿¡æ¯"""
    print(f"--- [{name}] Gradient Info ---")
    if tensor.grad is not None:
        print(f"Shape: {tensor.grad.shape}")
        print(f"Values: {tensor.grad}")
    else:
        print("None (æ— æ¢¯åº¦)")
    print("-" * 30)

print("ğŸš€ Day 8: Autograd é—­ç¯è®­ç»ƒå¼€å§‹\n")

# ==========================================
# 1. å‡†å¤‡æ•°æ® (requires_grad=True æ˜¯æ ¸å¿ƒ)
# ==========================================
# è¿™æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º (3,) çš„å‘é‡
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
print(f"è¾“å…¥ x: {x}\n")

# ==========================================
# åœºæ™¯ A: æ­£å¸¸åå‘ä¼ æ’­ (The Happy Path)
# è®¡ç®—å›¾: x -> y (å¹³æ–¹) -> z (ä¹˜æ³•) -> loss (æ±‚å’Œ)
# ==========================================
print(">>> åœºæ™¯ A: æ­£å¸¸åå‘ä¼ æ’­")

y = x ** 2        # ç®—å­1: PowerBackward
z = y * 4         # ç®—å­2: MulBackward
loss = z.sum()    # ç»ˆç‚¹: SumBackward

# åå‘ä¼ æ’­å‰ï¼Œæ¸…ç©ºæ¢¯åº¦ï¼ˆè™½ç„¶è¿™é‡Œæ˜¯ç¬¬ä¸€æ¬¡ï¼Œä½†å…»æˆå¥½ä¹ æƒ¯ï¼‰
if x.grad is not None: x.grad.zero_()

loss.backward()   # ğŸš€ å¯åŠ¨å¼•æ“ï¼

print(f"è®¡ç®—è¿‡ç¨‹: loss = sum((x^2) * 4)")
# æ•°å­¦æ¨å¯¼: 
# z = 4x^2 -> dz/dx = 8x
# x=[1,2,3] -> grad=[8, 16, 24]
print_grad_info(x, "x (Normal)")


# ==========================================
# åœºæ™¯ B: ä½¿ç”¨ .detach() (æ¢¯åº¦æˆªæ–­/å‰ªæ)
# ==========================================
print("\n>>> åœºæ™¯ B: ä½¿ç”¨ .detach() æˆªæ–­æ¢¯åº¦")

# é‡ç½® x çš„æ¢¯åº¦
x.grad.zero_()

y = x ** 2
y_detached = y.detach()  # âœ‚ï¸ æ¢¯åº¦åœ¨è¿™é‡Œæ–­å¼€äº†
z = y_detached * 4
loss = z.sum()

# --- ä¿®æ”¹å¼€å§‹ ---
if loss.requires_grad:
    loss.backward()
else:
    print("âš ï¸ æç¤º: loss.requires_grad ä¸º Falseï¼Œæ— æ³•è¿›è¡Œ backward()")
    print("è¿™è¯æ˜ detach() æˆåŠŸåˆ‡æ–­äº†è®¡ç®—å›¾ï¼")
# --- ä¿®æ”¹ç»“æŸ ---

# éªŒè¯ x çš„æ¢¯åº¦ï¼ˆåº”è¯¥æ˜¯ 0ï¼Œæˆ–è€…ä¿æŒä¸ºè¢« zero_() åçš„çŠ¶æ€ï¼‰
print_grad_info(x, "x (After detach)")

# ==========================================
# åœºæ™¯ C: ä½¿ç”¨ with torch.no_grad() (é—­çœ¼æ¨¡å¼)
# æ•´ä¸ªä¸Šä¸‹æ–‡éƒ½ä¸è¿½è¸ªæ¢¯åº¦ï¼Œå¸¸ç”¨äºæ¨ç†/æµ‹è¯•
# ==========================================
print("\n>>> åœºæ™¯ C: ä½¿ç”¨ torch.no_grad()")

with torch.no_grad():
    y = x ** 2
    z = y * 4
    loss = z.sum()
    
    print(f"Loss requires_gradçŠ¶æ€: {loss.requires_grad}") # åº”è¯¥æ˜¯ False
    
    try:
        loss.backward()
    except RuntimeError as e:
        print(f"âŒ æŠ¥é”™æ•è·: {e}")
        print("åŸå› : æ ¹æœ¬å°±æ²¡æœ‰æ„å»ºè®¡ç®—å›¾ï¼Œæ— æ³• backward")

print("\nâœ… Day 8 è®­ç»ƒå®Œæˆï¼")